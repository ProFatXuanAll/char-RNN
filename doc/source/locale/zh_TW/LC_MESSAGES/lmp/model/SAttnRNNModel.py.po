# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, ProFatXuanAll
# This file is distributed under the same license as the Language Model
# Playground package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Language Model Playground 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-13 16:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/lmp/model/SAttnRNNModel.py.rst:2
msgid ":py:class:`lmp.model.SAttnRNNModel`"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:1 of
msgid "RNN language model with self attention mechanism."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:3 of
msgid ""
"Same architecture as :py:class:`lmp.model.RNNModel` but use self "
"attention on RNN layer."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock
#: lmp.model._sattn_rnn.SAttnRNNBlock.forward
#: lmp.model._sattn_rnn.SAttnRNNModel
#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask
#: lmp.model._sattn_rnn.SAttnRNNModel.forward of
msgid "Parameters"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:6 of
msgid "Token embedding dimension. Must be bigger than or equal to ``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:9 of
msgid ""
"Hidden dimension for Feed-Forward layers and self attention RNN layers. "
"Must be bigger than or equal to ``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:49 lmp.model._sattn_rnn.SAttnRNNModel:12
#: of
msgid "Useless parameter. Intently left for subclass parameters extension."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:52 lmp.model._sattn_rnn.SAttnRNNModel:15
#: of
msgid ""
"Number of self attention RNN layers. Must be bigger than or equal to "
"``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:18 of
msgid ""
"Number of Feed-Forward layers after self attention RNN layers. All layers"
" are paired with ReLU activatons except for the last one. Must be bigger "
"than or equal to ``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:22 of
msgid ""
"Number of Feed-Forward layers before self attention RNN layers. All "
"layers are paired with ReLU activatons. Must be bigger than or equal to "
"``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:26 of
msgid ""
"Dropout probability for token embeddings. Must satisfy ``0.0 <= p_emb <= "
"1.0``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:55 lmp.model._sattn_rnn.SAttnRNNModel:29
#: of
msgid ""
"Dropout probability for every hidden representations. Must satisfy ``0.0 "
"<= p_hid <= 1.0``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:32 of
msgid "Tokenizer instance with attributes ``pad_tkid`` and ``vocab_size``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:37 of
msgid "Token embedding lookup matrix. Use token ids to lookup token embeddings."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock lmp.model._sattn_rnn.SAttnRNNModel of
msgid "type"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:40 of
msgid "torch.nn.Embedding"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:44 of
msgid ""
"Token embedding dropout. Drop embedding features with probability "
"``p_emb``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:47 of
msgid "torch.nn.Dropout"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:51 of
msgid ""
"Self attention RNN which encode temporal features. Each time step's "
"hidden state depends on current input and previous hidden state. Drop "
"temporal features with probability ``p_hid``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:56 of
msgid "lmp.model.SAttnRNNBlock"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:60 of
msgid "Model name is ``sattn-RNN``. Used for command line argument parsing."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:63 of
msgid "ClassVar[str]"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:67 of
msgid "Padding token id. Used to create attention mask on padding tokens."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:70 of
msgid "int"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:74 of
msgid ""
"Rectified Feed-Forward layers which transform temporal features from "
"hidden dimension ``d_hid`` to embedding dimension ``d_emb``. Drop "
"rectified units with probability ``p_hid``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:78 lmp.model._sattn_rnn.SAttnRNNModel:86
#: of
msgid "torch.nn.Sequential"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel:82 of
msgid ""
"Rectified Feed-Forward layers which transform token embeddings from "
"embedding dimension ``d_emb`` to hidden dimension ``d_hid``. Drop "
"rectified units with probability ``p_hid``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:1 of
msgid "Create self attention masks for ``batch_prev_tkids``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:3 of
msgid "Self attention masks are created as follow:"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:5 of
msgid ""
"Create auto-regressive self attention masks (mask everything above "
"diagnoal). This is needed since at each time step current input can only "
"see previous inputs and itself. (shape: ``(1, S, S)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:10 of
msgid ""
"Create padding self attention masks (mask every padding token id "
"positions). This is needed since paddings are meaningless. (shape: ``(B, "
"S, 1)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:14 of
msgid ""
"Perform ``or`` operation on auto-regressive self attention masks and "
"padding self attention masks. Return ``or`` operation results. (shape: "
"``(B, S, S)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:19
#: lmp.model._sattn_rnn.SAttnRNNModel.forward:30 of
msgid ""
"Batch of previous token ids encoded by :py:class:`lmp.tknzr.BaseTknzr` "
"subclass instance. ``batch_prev_tkids`` has shape ``(B, S)`` and ``dtype "
"== torch.int64``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward
#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask
#: lmp.model._sattn_rnn.SAttnRNNModel.forward of
msgid "Returns"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask:25 of
msgid "Self attention masks with shape ``(B, S, S)`` and ``dtype == torch.bool``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward
#: lmp.model._sattn_rnn.SAttnRNNModel.create_mask
#: lmp.model._sattn_rnn.SAttnRNNModel.forward of
msgid "Return type"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:1
#: lmp.model._sattn_rnn.SAttnRNNModel.forward:1 of
msgid "Perform forward pass."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:3
#: lmp.model._sattn_rnn.SAttnRNNModel.forward:3 of
msgid "Forward pass algorithm is structured as follow:"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:5 of
msgid "Input batch of previous token ids. (shape: ``(B, S)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:7 of
msgid ""
"Use batch of previous token ids to perform token embeddings lookup on "
"``self.emb``. (shape: ``(B, S, E)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:10 of
msgid ""
"Use ``self.emb_dp`` to drop some features in token embeddings. (shape: "
"``(B, S, E)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:12 of
msgid ""
"Use ``self.pre_hid`` to transform token embeddings from embedding "
"dimension ``E`` to hidden dimension ``H``. (shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:15 of
msgid "Use ``self.hid`` to encode temporal features. (shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:17 of
msgid ""
"Use ``self.post_hid`` to transform temporal features from hidden "
"dimension ``H`` to embedding dimension ``E``. (shape: ``(B, S, E)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:20 of
msgid ""
"Find the most possible next token id in embedding matrix ``self.emb`` "
"using inner product. This reduce parameters since we share weight on "
"token embedding and output projection. (shape: ``(B, S, V)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:25 of
msgid ""
"Return logits. Used with ``self.pred`` to convert logit into prediction. "
"Used wtih ``self.loss_fn`` to perform optimization. (shape: ``(B, S, "
"V)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNModel.forward:36 of
msgid ""
"Next token logits for each token id in batch. Logits has shape ``(B, S, "
"V)`` and ``dtype == torch.float32``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:1 of
msgid "RNN block with self attention mechanism."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:3 of
msgid ""
"Each output of RNN will be used to calculate self attention scores. Self "
"attention scores are calculated as follow (see [Vaswani2017]_ for more "
"details on self attention scores):"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:7 of
msgid ""
"\\begin{align*}\n"
"t &\\in [1, S] \\\\\n"
"l &\\in [1, L] \\\\\n"
"x_{1:S}^l &= x_1^l, \\dots, x_S^l \\\\\n"
"h_0^l &= 0 \\\\\n"
"h_t^l &= \\text{RNN}(x_t^l, h_{t-1}^l) \\\\\n"
"Q^l &= W_Q^l h_{1:S}^l + b_Q^l \\\\\n"
"K^l &= W_K^l h_{1:S}^l + b_K^l \\\\\n"
"V^l &= W_V^l h_{1:S}^l + b_V^l \\\\\n"
"A^l &= \\text{softmax}(\\frac{Q^l (K^{l})^{\\top}}{\\sqrt{S}})V^l \\\\\n"
"y_{1:S}^l &= W_O^l A^l + b_O^l \\\\\n"
"x_{1:S}^{l+1} &= y_{1:S}^l\n"
"\\end{align*}"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:23 of
msgid ""
"Where :math:`x_{1:S}^l` is the input sequeence with length :math:`S` at "
"layer :math:`l`, :math:`L` means number of layer (same as ``n_hid_lyr``)."
" :math:`W_Q^l, W_K^l, W_V^l, W_O^l` and :math:`b_Q^l, b_K^l, b_V^l, "
"b_O^l` are linear transformation weights and biases for query, key, "
"value, and output at layer :math:`l`, respectively. :math:`A^l` is self "
"attention scores weighted sum. :math:`x_t^l` means input sequence time "
"step :math:`t` at layer :math:`l`, :math:`h_t^l` means hidden "
"representation encoded by recurrent layer :math:`l`, :math:`h_0^l` means "
"initial hidden representation of recurrent layer :math:`l`, "
":math:`y_{1:t}^l` is the output of layer :math:`l`."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:34 of
msgid ""
"Each output of RNN will then use self attention scores as weights to "
"calculate weighted sum as final output. No residual connection is used."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:38 of
msgid ""
"For comment throughout this class and its subclasses, we use the "
"following symbols to denote the shape of tensors:"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:41 of
msgid "``B``: Batch size."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:42 of
msgid "``H``: Hidden representation dimension."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:43 of
msgid "``S``: Length of sequence of tokens."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:45 of
msgid ""
"Hidden dimension for RNN and self attention linear transformation weights"
" (including query, key, value and output). Must be bigger than or equal "
"to ``1``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:61 of
msgid ""
"Drop each output temporal features of ``self.recur`` with probability "
"``p_hid``. Do not dropout last temporal features output from "
"``self.recur[-1]`` since :py:class:`lmp.model.SAttnRNNModel` have "
"``self.post_hid`` which drop output of ``self.hid``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:67 of
msgid "torch.nn.ModuleList[torch.nn.Dropout]"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:71 of
msgid ""
"Linear transformation which transform temporal features to key (query "
"target) features. See [Vaswani2017]_ for details on self attention key."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:75 lmp.model._sattn_rnn.SAttnRNNBlock:85
#: lmp.model._sattn_rnn.SAttnRNNBlock:93 lmp.model._sattn_rnn.SAttnRNNBlock:109
#: of
msgid "torch.nn.ModuleList[torch.nn.Linear]"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:79 of
msgid ""
"Final linear transformation after weighted sum using attention scores. Do"
" not dropout last output features from ``self.out[-1]`` since "
":py:class:`lmp.model.SAttnRNNModel` have ``self.post_hid`` which drop "
"output of ``self.hid``. See [Vaswani2017]_ for details on multi-heads "
"self attention."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:89 of
msgid ""
"Linear transformation which transform temporal features to query "
"features. See [Vaswani2017]_ for details on self attention query."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:97 of
msgid ""
"List of vanilla RNN which encode temporal features. Each time step's "
"hidden state depends on current input and previous hidden state."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:101 of
msgid "torch.nn.ModuleList[torch.nn.RNN]"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:105 of
msgid ""
"Linear transformation which transform temporal features to attention "
"scores weighted sum features. See [Vaswani2017]_ for details on self "
"attention value."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:112 of
msgid "References"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock:113 of
msgid ""
"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, "
"Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017. Attention is all "
"you need."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:5 of
msgid ""
"Input batch of previous token hidden representations. (shape: ``(B, S, "
"H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:7 of
msgid "Use for-loop to perform the following operations:"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:9 of
msgid "Use recurrent layer to encode temporal features. (shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:11 of
msgid ""
"Calculate query, key and value features on recurrent layer output. "
"(shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:14 of
msgid ""
"Calculate self attention scores with query and key features. (shape: "
"``(B, S, S)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:16 of
msgid ""
"Mask parts of self attention scores by replacing masked positions with "
"large negative value. (shape: ``(B, S, S)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:19 of
msgid ""
"Use self attention scores to as weights to calculate weighted sum on "
"value features. (shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:22 of
msgid ""
"Perform one more linear transformation on weighted sum features. (shape: "
"``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:24 of
msgid ""
"Drop some features. This step is skipped on last for loop step. (shape: "
"``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:27 of
msgid "Use sparse features as input of next loop."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:29 of
msgid "Return final output. (shape: ``(B, S, H)``)"
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:32 of
msgid ""
"Batch of attention mask. ``batch_tk_mask`` has shape ``(B, S, S)`` and "
"``dtype == torch.bool``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:36 of
msgid ""
"Batch of previous token hidden representation. ``batch_tk_reps`` has "
"shape ``(B, S, H)`` and ``dtype == torch.float32``."
msgstr ""

#: lmp.model._sattn_rnn.SAttnRNNBlock.forward:41 of
msgid ""
"Self attention recurrent features with shape ``(B, S, H)`` and ``dtype =="
" torch.float32``."
msgstr ""

