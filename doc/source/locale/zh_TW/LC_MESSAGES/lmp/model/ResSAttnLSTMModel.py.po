# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, ProFatXuanAll
# This file is distributed under the same license as the Language Model
# Playground package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Language Model Playground 1.0.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-13 16:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/lmp/model/ResSAttnLSTMModel.py.rst:2
msgid ":py:class:`lmp.model.ResSAttnLSTMModel`"
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:1 of
msgid "Residual connected LSTM language model with self attention mechanism."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:3 of
msgid ""
"Same architecture as :py:class:`lmp.model.ResSAttnRNNModel` but replace "
"residual connected self attention RNN with residual connected self "
"attention LSTM instead."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock
#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel of
msgid "Parameters"
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:7 of
msgid "Token embedding dimension. Must be bigger than or equal to ``1``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:10 of
msgid ""
"Hidden dimension for Feed-Forward layers and residual connected self "
"attention LSTM layers. Must be bigger than or equal to ``1``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:10
#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:14 of
msgid "Useless parameter. Intently left for subclass parameters extension."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:13
#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:17 of
msgid ""
"Number of residual connected self attention LSTM layers. Must be bigger "
"than or equal to ``1``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:20 of
msgid ""
"Number of Feed-Forward layers after residual connected self attention "
"LSTM layers. All layers are paired with ReLU activatons except for the "
"last one. Must be bigger than or equal to ``1``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:25 of
msgid ""
"Number of Feed-Forward layers before residual connected self attention "
"LSTM layers. All layers are paired with ReLU activatons. Must be bigger "
"than or equal to ``1``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:30 of
msgid ""
"Dropout probability for token embeddings. Must satisfy ``0.0 <= p_emb <= "
"1.0``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:16
#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:33 of
msgid ""
"Dropout probability for every hidden representations. Must satisfy ``0.0 "
"<= p_hid <= 1.0``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:36 of
msgid "Tokenizer instance with attributes ``pad_tkid`` and ``vocab_size``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:41 of
msgid ""
"Self attention LSTM with residual connection which encode temporal "
"features. Each time step's hidden state depends on current input and "
"previous hidden state. Drop temporal features with probability ``p_hid``."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel of
msgid "type"
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:47 of
msgid "lmp.model.SAttnLSTMBlock"
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:51 of
msgid "Model name is ``res-sattn-LSTM``. Used for command line argument parsing."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMModel:54 of
msgid "ClassVar[str]"
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:1 of
msgid "Residual connected LSTM block with self attention mechanism."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:3 of
msgid ""
"Same architecture as :py:class:`lmp.model.ResSAttnRNNBlock` but replace "
"RNN with LSTM instead."
msgstr ""

#: lmp.model._res_sattn_lstm.ResSAttnLSTMBlock:6 of
msgid ""
"Hidden dimension for LSTM and self attention linear transformation "
"weights (including query, key, value and output). Must be bigger than or "
"equal to ``1``."
msgstr ""

